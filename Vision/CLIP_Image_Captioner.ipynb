{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSdYVVCYY8Oe"
      },
      "source": [
        "## Image Captioner\n",
        "We explore how to build a captioning model. It will rely on the CLIP model to perform inference. We will then use `gradio` to deploy it as a web app."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Aojl9R-Utvq"
      },
      "source": [
        "## Setting up Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3KHCG4RTKAf",
        "outputId": "56ac7521-eb61-4167-bc6b-98bf7268974f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'nvcc' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# Checking CUDA version.\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxESF4VXUgTp"
      },
      "outputs": [],
      "source": [
        "# !conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.8\n",
        "# !pip install ftfy regex tqdm\n",
        "# !pip install git+https://github.com/openai/CLIP.git\n",
        "# !pip install gradio==4.7.1\n",
        "# !pip install typing_extensions==4.7.1 --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-h96bd_m5\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "     -------------------------------------- 53.4/53.4 KB 345.2 kB/s eta 0:00:00\n",
            "Collecting regex\n",
            "  Downloading regex-2023.10.3-cp310-cp310-win_amd64.whl (269 kB)\n",
            "     ------------------------------------ 269.6/269.6 KB 922.8 kB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from clip==1.0) (2.0.1)\n",
            "Requirement already satisfied: torchvision in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from clip==1.0) (0.15.2)\n",
            "Collecting wcwidth<0.3.0,>=0.2.12\n",
            "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: jinja2 in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: networkx in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: sympy in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: typing-extensions in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: filelock in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from torch->clip==1.0) (3.12.2)\n",
            "Requirement already satisfied: requests in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from torchvision->clip==1.0) (2.30.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from torchvision->clip==1.0) (9.5.0)\n",
            "Requirement already satisfied: numpy in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: colorama in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from requests->torchvision->clip==1.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from requests->torchvision->clip==1.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from requests->torchvision->clip==1.0) (3.1.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py): started\n",
            "  Building wheel for clip (setup.py): finished with status 'done'\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369620 sha256=5e545bb0b3b9a66efa528b39288fc5ed76a445609e4c234bcc36aee2a3922288\n",
            "  Stored in directory: C:\\Users\\USER\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-mlm9n9_g\\wheels\\da\\2b\\4c\\d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: wcwidth, regex, ftfy, clip\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.6\n",
            "    Uninstalling wcwidth-0.2.6:\n",
            "      Successfully uninstalled wcwidth-0.2.6\n",
            "Successfully installed clip-1.0 ftfy-6.1.3 regex-2023.10.3 wcwidth-0.2.12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\USER\\AppData\\Local\\Temp\\pip-req-build-h96bd_m5'\n",
            "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
            "You should consider upgrading via the 'F:\\Jupyter\\DeepLearning\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: typing_extensions==4.7.1 in f:\\jupyter\\deeplearning\\venv\\lib\\site-packages (4.7.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
            "You should consider upgrading via the 'F:\\Jupyter\\DeepLearning\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install typing_extensions==4.7.1 --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "POiUSGlFZyDE",
        "outputId": "2ead9ae6-3d2e-481b-9c50-e16b88f9cae8"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAOfAMTLZ1sm"
      },
      "outputs": [],
      "source": [
        "image = Image.open(\"data/dog.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi1IJymLYbBr"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Download the dataset\n",
        "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJNo3fvLaUFI"
      },
      "outputs": [],
      "source": [
        "image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V37_xdPbY2Kk",
        "outputId": "028410d0-e93f-4b7b-a6ec-57984546a029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top predictions:\n",
            "\n",
            "             fox: 18.66%\n",
            "    sweet_pepper: 10.32%\n",
            "            girl: 7.12%\n",
            "             boy: 6.09%\n",
            "            wolf: 5.54%\n"
          ]
        }
      ],
      "source": [
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "DeepLearning",
      "language": "python",
      "name": "deeplearning"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
