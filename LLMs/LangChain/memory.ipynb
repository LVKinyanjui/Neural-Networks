{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large language models are `stateless`. Each call is independent.\n",
    "They only appear to have memory by providing each call with a **context**. This is what gives them memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models.google_palm import ChatGooglePalm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = os.getenv(\"PALM_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Count Errors\n",
    "There is a potential error that langchain could raise when you ask for a prediction. This is `ChatGooglePalmError`. The reason this is happening is that when temperature is set to low values, the llm returns `None`. Hence why it says: **ChatResponse must have at least one candidate**. \n",
    "\n",
    "> candidate_count: The **maximum** number of generated response messages to return.\n",
    "    \n",
    "            This value must be between `[1, 8]`, inclusive. If unset, this\n",
    "            will default to `1`.\n",
    "    \n",
    "            Note: Only unique candidates are returned. Higher temperatures are more\n",
    "            likely to produce unique candidates. Setting `temperature=0.0` will always\n",
    "            return 1 candidate regardless of the `candidate_count`.\n",
    "\n",
    "The patch for now is to set temperature high.\n",
    "\n",
    "Also setting k to high seems to also mitigate this error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k (parameter)\n",
    "Determines how much is remembered. It takes an integer specifying the number of **conversational exchanges** to be remembered.\n",
    "By default `k=5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferWindow\n",
    "Provides a window of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGooglePalm(google_api_key=api_key, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initialize ConversationBufferWindowMemory\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "\n",
    "# Load memory variables\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ConversationChain\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: What is my name?\n",
      "AI: I do not know your name. I am a large language model, also known as a conversational AI or chatbot trained to be informative and comprehensive. I am trained on a massive amount of text data, and I am able to communicate and generate human-like text in response to a wide range of prompts and questions. For example, I can provide summaries of factual topics or create stories.\n",
      "\n",
      "However, I am not able to access your personal information, such as your name. This is because I am not trained on personal data, and I am not able to track or store personal information. I am only able to access and process information that is publicly available.\n",
      "\n",
      "If you would like to share your name with me, I would be happy to learn it. However, I understand if you would prefer to keep your name private.\n",
      "Human: My name is Andrew?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you Andrew! It's a pleasure to chat with you.\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"My name is Andrew?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationTokenBufferMemory\n",
    "Allows us to specify a max token value.\n",
    "\n",
    "Requires the **transformers** library to work. Install it, if already not installed using, `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGooglePalm(google_api_key=api_key, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"AI: Hello, Andrew! It's nice to meet you. How can I help you today?\\nHuman: What is 1+1?\\nAI: Hi Andrew, it's nice to meet you too!\\r\\n\\r\\n1+1 is 2.\\nHuman: What is my name?\\nAI: Hi Andrew! It's nice to meet you again.\\r\\n\\r\\nYour name is Andrew.\\nHuman: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "AI: Hello, Andrew! It's nice to meet you. How can I help you today?\n",
      "Human: What is 1+1?\n",
      "AI: Hi Andrew, it's nice to meet you too!\n",
      "\n",
      "1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Hi Andrew! It's nice to meet you again.\n",
      "\n",
      "Your name is Andrew.\n",
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "AI: Hi Andrew, it's nice to meet you too!\n",
      "\n",
      "1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Hi Andrew! It's nice to meet you again.\n",
      "\n",
      "Your name is Andrew.\n",
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hi Andrew! It's nice to meet you again.\n",
      "\n",
      "What's up?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is my name?\n",
      "AI: Hi Andrew! It's nice to meet you again.\n",
      "\n",
      "Your name is Andrew.\n",
      "Human: Hi\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hi Andrew! It's nice to meet you again.\n",
      "\n",
      "What's up?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1=2.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Andrew, it's nice to meet you again. What can I help you with today?\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")\n",
    "conversation.predict(input=\"What is 1+1?\")\n",
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGooglePalm(google_api_key=api_key, temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: Today, we will be discussing the future of artificial intelligence. We will explore the potential benefits and risks of AI, and discuss how we can ensure that AI is used for good.\\nHuman: That sounds interesting. I'm looking forward to it.\\nAI: Me too.\\n\\nThe human and AI discuss the future of artificial intelligence. They explore the potential benefits and risks of AI, and discuss how we can ensure that AI is used for good. The human is interested in the topic, and the AI is knowledgeable and engaging. The conversation is productive and informative.\\nAI: There is a meeting at 8am with your product team. You will need your PowerPoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the Italian restaurant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \" \\\n",
    "           \"You will need your PowerPoint presentation prepared. \" \\\n",
    "           \"9am-12pm have time to work on your LangChain \" \\\n",
    "           \"project which will go quickly because Langchain is such a powerful tool. \" \\\n",
    "           \"At Noon, lunch at the Italian restaurant with a customer who is driving \" \\\n",
    "           \"from over an hour away to meet you to understand the latest in AI. \" \\\n",
    "           \"Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm, memory=memory,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: Human: Hello\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: What is on the schedule today?\n",
      "AI: Today, we will be discussing the future of artificial intelligence. We will explore the potential benefits and risks of AI, and discuss how we can ensure that AI is used for good.\n",
      "Human: That sounds interesting. I'm looking forward to it.\n",
      "AI: Me too.\n",
      "\n",
      "The human and AI discuss the future of artificial intelligence. They explore the potential benefits and risks of AI, and discuss how we can ensure that AI is used for good. The human is interested in the topic, and the AI is knowledgeable and engaging. The conversation is productive and informative.\n",
      "AI: There is a meeting at 8am with your product team. You will need your PowerPoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the Italian restaurant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"There are many different types of LLM demos that you could show your customer. Here are a few ideas:\\n\\n* You could show a demo of the LLM being used to generate text. For example, you could ask the LLM to write a poem, a news article, or a blog post.\\n* You could show a demo of the LLM being used to translate text from one language to another. For example, you could ask the LLM to translate a sentence from English to Spanish or vice versa.\\n* You could show a demo of the LLM being used to answer questions. For example, you could ask the LLM questions about a particular topic and see how well it can answer them.\\n* You could show a demo of the LLM being used to generate creative content. For example, you could ask the LLM to write a song, a story, or a piece of art.\\n\\nThe best demo to show will depend on your customer's interests and needs. If you are not sure what to show, you could always ask your customer what they would like to see.\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "The procedure of providing memory to the llm is as follows:\n",
    "\n",
    "1. Initialize the llm\n",
    "`llm = ChatGooglePalm(google_api_key=api_key, temperature=0.7)`\n",
    "\n",
    "2. Initialize memory buffer\n",
    "`memory = ConversationBufferMemory()`\n",
    "\n",
    "3. Add context to memory, just initlialized\n",
    "```\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "```\n",
    "\n",
    "4. Initialize conversation chain\n",
    "`conversation = ConversationChain(llm=llm, memory=memory,verbose=True)`\n",
    "\n",
    "5. Ask the llm for a response.\n",
    "`conversation.predict(input=\"What is my name?\")`\n",
    "\n",
    "**NB**: It helps greatly if what you ask is related to the context you provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Google Palm is quite finicky as it will sometimes return `None`, rather than a text response which causes the langchain wrapper to raise a `ChatGooglePalmError`.\n",
    "\n",
    "We can reduce the chances of getting no response by making sure the context is sufficient for the llm to make a response.\n",
    "We do this by:\n",
    "\n",
    "1. Setting temperature to high\n",
    "2. Setting k to a hiigher value (That way it remembers more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
