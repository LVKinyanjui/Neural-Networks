{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"data/sample.txt\", encoding='utf-8')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import GooglePalmEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "palm_api_key = os.getenv(\"PALM_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GooglePalmEmbeddings(google_api_key=palm_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores\n",
    "These allow us to store our text embeddings for querying later. In this notebook we will be using `pinecone`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=os.getenv(\"PINECONE_API_KEY_02\"),  # find at app.pinecone.io\n",
    "    environment=os.getenv(\"PINECONE_ENV\"),  # next to api key in console\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_name = \"langchain-demo\"\n",
    "\n",
    "# First, check if our index already exists. If it doesn't, we create it\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # we create a new index\n",
    "    pinecone.create_index(name=index_name, metric=\"cosine\", dimension=768)\n",
    "# The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions`\n",
    "docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
    "\n",
    "# if you already have an index, you can load it like this\n",
    "# docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "\n",
    "query = \"What is the point of this text?\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 768,\n",
       " 'index_fullness': 0.00074,\n",
       " 'namespaces': {'': {'vector_count': 74}},\n",
       " 'total_vector_count': 74}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.GRPCIndex(\"langchain-demo\")\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GooglePalm(google_api_key=palm_api_key, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the procedure of annotating a vehicle?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuff Method\n",
    "In this section we explore retrieval using the `stuff` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The procedure of annotating a vehicle is as follows:\n",
       "\n",
       "1. Create a 3D bounding box for the vehicle, based on available LiDAR measurements, in combination with vision based information from the image. If there is no LiDAR data, the 3D bounding box should still be annotated and the vehicle size and position in the world should be estimated according to the table below.\n",
       "2. Each task will contain around 300 frames and each object should be tracked with a single ID.\n",
       "3. Each bounding box should be drawn as tightly as possible to the object.\n",
       "4. Please take the following steps to avoid making errors and to ensure highest quality:\n",
       "    * Use the most specific label possible for each object.\n",
       "    * Make sure that the object is completely inside the bounding box.\n",
       "    * Do not overlap bounding boxes.\n",
       "    * Do not draw bounding boxes around parts of objects.\n",
       "5. If the cargo is a vehicle, it should be annotated as a NondescriptVehicle object. Cargo should otherwise not be annotated.\n",
       "6. For release validation, the cargo should not be annotated, even if the cargo is a vehicle."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = qa_stuff.run(query)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Retrieval Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce\n",
    "Gets a response for each chunk and question pair then uses another llm call to summarize the responses.\n",
    "Good for recursive document summarization.\n",
    "\n",
    "- Requires more calls\n",
    "- Assumes documents (chunks) are independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"map_reduce\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The procedure of annotating a vehicle is as follows:\n",
       "\n",
       "1. Create a 3D bounding box for the vehicle, based on available LiDAR measurements, in combination with vision based information from the image. If there is no LiDAR data, the 3D bounding box should still be annotated and the vehicle size and position in the world should be estimated according to the table below.\n",
       "2. Each task will contain around 300 frames and each object should be tracked with a single ID.\n",
       "3. Each bounding box should be drawn as tightly as possible to the object.\n",
       "4. Please take the following steps to avoid making errors and to ensure highest quality:\n",
       "    * Use the most specific label possible for each object.\n",
       "    * Make sure that the object is completely inside the bounding box.\n",
       "    * Do not overlap bounding boxes.\n",
       "    * Do not draw bounding boxes around parts of objects.\n",
       "5. If the cargo is a vehicle, it should be annotated as a NondescriptVehicle object. Cargo should otherwise not be annotated.\n",
       "6. For release validation, the cargo should not be annotated, even if the cargo is a vehicle."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = qa_stuff.run(query)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refine\n",
    "Iteratively builds answer from past calls (like a cascade).\n",
    "\n",
    "- Longer answer\n",
    "- Takes longer (slower)\n",
    "- At least as many calls as map reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"refine\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The procedure of annotating a vehicle is as follows:\n",
       "\n",
       "1. Create a 3D bounding box for the vehicle, based on available LiDAR measurements, in combination with vision based information from the image. If there is no LiDAR data, the 3D bounding box should still be annotated and the vehicle size and position in the world should be estimated according to the table below.\n",
       "2. Each task will contain around 300 frames and each object should be tracked with a single ID.\n",
       "3. Each bounding box should be drawn as tightly as possible to the object.\n",
       "4. Please take the following steps to avoid making errors and to ensure highest quality:\n",
       "    * Use the most specific label possible for each object.\n",
       "    * Make sure that the object is completely inside the bounding box.\n",
       "    * Do not overlap bounding boxes.\n",
       "    * Do not draw bounding boxes around parts of objects.\n",
       "5. If the cargo is a vehicle, it should be annotated as a NondescriptVehicle object. Cargo should otherwise not be annotated.\n",
       "6. For release validation, the cargo should not be annotated, even if the cargo is a vehicle."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = qa_stuff.run(query)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Rerank (Experimental)\n",
    "Single call for each document returning a score from which we specify the highest score to select.\n",
    "- Faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
